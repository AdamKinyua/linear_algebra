{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b37767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd1b1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROBENIUS NORM\n",
      "\n",
      "- Allows us to quantify the size of a matrix\n",
      "- Sum of the magnitude of all the vectors in matrix X\n",
      "\n",
      "5.477225575051661\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "PROBENIUS NORM\n",
    "\n",
    "- Allows us to quantify the size of a matrix\n",
    "- Sum of the magnitude of all the vectors in matrix X\n",
    "\"\"\")\n",
    "\n",
    "x = np.array([[1,2], [3,4]])\n",
    "\n",
    "#print(proNorm(x))\n",
    "\n",
    "proNormFunction = np.linalg.norm(x)\n",
    "print(proNormFunction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bcfd819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATRIX INVERSION\n",
      "\n",
      "-This is a clever convinient approach for solving linear equations computationally\n",
      "- An alternative to manually solving with substitution and elimination\n",
      "- Matrix inverse of X equals X^-1 whereby X times X^-1 = Identity matrix\n",
      "\n",
      "regression formula can be summarized as:\n",
      "\n",
      "y = Xw where y is vector tensor, X is a matrix tensor, and w is a weight tensor of weights a through ....m maybe\n",
      "y is the outcome, maybe a house price \n",
      "X is the predictors, maybe of price\n",
      "w contains the unknown that models the learnable parameters\n",
      "\n",
      "Assuming that X^-1 exists, then matrix inversion can solve for w\n",
      "\n",
      "y = Xw\n",
      "Xw = y\n",
      "---------multiplying both sides of the equation by the inverse of the matrix\n",
      "X^-1.Xw = X^-1.y\n",
      "Iw = X^-1.y\n",
      "w = X^-1.y : here lies the solution to vector w\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIMITATIONS TO WHERE WE CAN APPLY MATRIX INVERSION\n",
      "-Can only be calculated if Matrix isn't singular. \n",
      "- i.e all colums of matrix must be linearly independent \n",
      "- Square matrix needed\n",
      "- Overdetermination: the number of rows is greater than the number of columns \n",
      "- Underdetermination: the number of rows is less than the number of columns \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "MATRIX INVERSION\n",
    "\n",
    "-This is a clever convinient approach for solving linear equations computationally\n",
    "- An alternative to manually solving with substitution and elimination\n",
    "- Matrix inverse of X equals X^-1 whereby X times X^-1 = Identity matrix\n",
    "\n",
    "regression formula can be summarized as:\n",
    "\n",
    "y = Xw where y is vector tensor, X is a matrix tensor, and w is a weight tensor of weights a through ....m maybe\n",
    "y is the outcome, maybe a house price \n",
    "X is the predictors, maybe of price\n",
    "w contains the unknown that models the learnable parameters\n",
    "\n",
    "Assuming that X^-1 exists, then matrix inversion can solve for w\n",
    "\n",
    "y = Xw\n",
    "Xw = y\n",
    "---------multiplying both sides of the equation by the inverse of the matrix\n",
    "X^-1.Xw = X^-1.y\n",
    "Iw = X^-1.y\n",
    "w = X^-1.y : here lies the solution to vector w\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "X = np.array([[4,2],[-5,-3]])\n",
    "xInverse = np.linalg.inv(X)\n",
    "y = np.array([4,-7])\n",
    "w = np.dot(xInverse, y) #performs X^-1.y; supports the answer arrived at\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "LIMITATIONS TO WHERE WE CAN APPLY MATRIX INVERSION\n",
    "-Can only be calculated if Matrix isn't singular. \n",
    "- i.e all colums of matrix must be linearly independent \n",
    "- Square matrix needed\n",
    "- Overdetermination: the number of rows is greater than the number of columns \n",
    "- Underdetermination: the number of rows is less than the number of columns \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07139373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIAGONAL MATRICES & ORTHOGONAL MATRICES\n",
      "\n",
      "-Non-zeroes in diagonal and zeroes everywhere else\n",
      "- Diagonal matrices are computationally efficient \n",
      "- Can be non-square \n",
      "\n",
      "ORTHOGONAL MATRICES - made up entirely of orthonormal vectors\n",
      "- The orthogonal vectors make up all rows and all columns \n",
      "- Therefore: A^T.A = AA^T=I where A^T is A transpose\n",
      "- This means that: A^T = A^-1.I = A^-1\n",
      "- Calculating A^T is cheap, therefore so is calclating A^-1\n",
      "\n",
      "\n",
      "Diag Matrix: \n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "\n",
      "\n",
      "Dot products of vectors in matrix above are:\n",
      " 0\n",
      "\n",
      " 0\n",
      "\n",
      " 0\n",
      "Norm of the vectors in our matrix are: \n",
      " 1.0\n",
      "\n",
      " 1.0\n",
      "\n",
      " 1.0\n",
      "\n",
      "Therefore, this particular matrix is orthonormal\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "DIAGONAL MATRICES & ORTHOGONAL MATRICES\n",
    "\n",
    "-Non-zeroes in diagonal and zeroes everywhere else\n",
    "- Diagonal matrices are computationally efficient \n",
    "- Can be non-square \n",
    "\n",
    "ORTHOGONAL MATRICES - made up entirely of orthonormal vectors\n",
    "- The orthogonal vectors make up all rows and all columns \n",
    "- Therefore: A^T.A = AA^T=I where A^T is A transpose\n",
    "- This means that: A^T = A^-1.I = A^-1\n",
    "- Calculating A^T is cheap, therefore so is calclating A^-1\n",
    "\n",
    "\"\"\")\n",
    "diagMatrix = np.array([[1,0,0], [0,1,0], [0,0,1]])\n",
    "print(f\"Diag Matrix: \\n{diagMatrix}\\n\\n\")\n",
    "column_1 = diagMatrix[:,0]\n",
    "column_2 = diagMatrix[:,1]\n",
    "column_3 = diagMatrix[:,2]\n",
    "print(f\"Dot products of vectors in matrix above are:\\n {np.dot(column_1, column_2)}\\n\\n {np.dot(column_1, column_3)}\\n\\n {np.dot(column_2,column_3)}\")\n",
    "print(f\"Norm of the vectors in our matrix are: \\n {np.linalg.norm(column_1)}\\n\\n {np.linalg.norm(column_2)}\\n\\n {np.linalg.norm(column_3)}\")\n",
    "print(\"\\nTherefore, this particular matrix is orthonormal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a27a792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.996169  0.       -0.025615  0.083619]\n",
      " [ 0.008305  0.707107  0.69768   0.114779]\n",
      " [ 0.008305 -0.707107  0.69768   0.114779]\n",
      " [-0.086662  0.       -0.160717  0.983188]]\n",
      "\n",
      "[[ 0.       -0.185853 -0.350276  0.918022  0.        0.        0.\n",
      "   0.      ]\n",
      " [ 0.707107 -0.304758  0.614253  0.172673  0.        0.        0.\n",
      "   0.      ]\n",
      " [-0.707107 -0.304758  0.614253  0.172673  0.        0.        0.\n",
      "   0.      ]\n",
      " [ 0.        0.883009  0.350276  0.312414  0.        0.        0.\n",
      "   0.      ]\n",
      " [ 0.        0.        0.        0.        1.        0.        0.\n",
      "   0.      ]\n",
      " [ 0.        0.        0.        0.        0.        1.        0.\n",
      "   0.      ]\n",
      " [ 0.        0.        0.        0.        0.        0.        1.\n",
      "   0.      ]\n",
      " [ 0.        0.        0.        0.        0.        0.        0.\n",
      "   1.      ]]\n"
     ]
    }
   ],
   "source": [
    "ci_eigenvectors_h3 = np.array([[0.996169,0.000000,-0.025615,0.083619],\n",
    "                               [0.008305,0.707107,0.697680,0.114779],\n",
    "                               [0.008305,-0.707107,0.697680,0.114779],\n",
    "                               [-0.086662,0.000000,-0.160717,0.983188]])\n",
    "ci_eigenvectors_h2 = np.array([[0.000000,-0.185853,-0.350276,0.918022,0.000000,0.000000,0.000000,0.000000],\n",
    "                              [0.707107,-0.304758,0.614253,0.172673,0.000000,0.000000,0.000000,0.000000],\n",
    "                              [-0.707107,-0.304758,0.614253,0.172673,0.000000,0.000000,0.000000,0.000000],\n",
    "                              [0.000000,0.883009,0.350276,0.312414,0.000000,0.000000,0.000000,0.000000],\n",
    "                              [0.000000,0.000000,0.000000,0.000000,1.000000,0.000000,0.000000,0.000000],\n",
    "                              [0.000000,0.000000,0.000000,0.000000,0.000000,1.000000,0.000000,0.000000],\n",
    "                              [0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,1.000000,0.000000],\n",
    "                              [0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,1.000000]])\n",
    "print(f\"{ci_eigenvectors_h3}\\n\\n{ci_eigenvectors_h2}\")\n",
    "#final_eigenVectors = np.dot(ci_eigenvectors_h3,ci_eigenvectors_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741952c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
